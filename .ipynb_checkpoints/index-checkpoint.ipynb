{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b24f4081",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91d04ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random # To generate random responses\n",
    "import string # For removing punctuation - data preprocessing\n",
    "import numpy as np # Handling arrays\n",
    "import io # Provides Pythonâ€™s main facilities for dealing with various types of I/O - text, binary and raw I/O\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # Data encoding\n",
    "from sklearn.metrics.pairwise import cosine_similarity # Similarity-based response generation\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer # For Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17449a7b",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "981ca7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize():\n",
    "    file = open('corpus.txt','r', errors = 'ignore')\n",
    "    corpus = file.read()\n",
    "    sentence_tokens = nltk.sent_tokenize(corpus)\n",
    "    word_tokens = nltk.word_tokenize(corpus)\n",
    "    return sentence_tokens, word_tokens\n",
    "    #print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b125349d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Artificial intelligence or AI is intelligence perceiving, synthesizing, and inferring informationn demonstrated by machines, as opposed to intelligence displayed by non human animals and humans.',\n",
       "  'Example tasks in which this is done include speech recognition, computer vision, translation between natural languages, as well as other mappings of inputs.',\n",
       "  'AI applications include advanced web search engines e.g., Google Search, recommendation systems used by YouTube, Amazon and Netflix, understanding human speech such as Siri and Alexa, self-driving cars e.g., Waymo, automated decision-making and competing at the highest level in strategic game systems such as chess and Go.',\n",
       "  'As machines become increasingly capable, tasks considered to require intelligence are often removed from the definition of AI, a phenomenon known as the AI effect.',\n",
       "  'For instance, optical character recognition is frequently excluded from things considered to be AI, having become a routine technology.',\n",
       "  'Artificial intelligence was founded as an academic discipline in 1956, and in the years since has experienced several waves of optimism, followed by disappointment and the loss of funding known as an AI winter followed by new approaches, success and renewed funding.',\n",
       "  'AI research has tried and discarded many different approaches since its founding, including simulating the brain, modeling human problem solving, formal logic, large databases of knowledge and imitating animal behavior.',\n",
       "  'In the first decades of the 21st century, highly mathematical-statistical machine learning has dominated the field, and this technique has proved highly successful, helping to solve many challenging problems throughout industry and academia.'],\n",
       " ['Artificial',\n",
       "  'intelligence',\n",
       "  'or',\n",
       "  'AI',\n",
       "  'is',\n",
       "  'intelligence',\n",
       "  'perceiving',\n",
       "  ',',\n",
       "  'synthesizing',\n",
       "  ',',\n",
       "  'and',\n",
       "  'inferring',\n",
       "  'informationn',\n",
       "  'demonstrated',\n",
       "  'by',\n",
       "  'machines',\n",
       "  ',',\n",
       "  'as',\n",
       "  'opposed',\n",
       "  'to',\n",
       "  'intelligence',\n",
       "  'displayed',\n",
       "  'by',\n",
       "  'non',\n",
       "  'human',\n",
       "  'animals',\n",
       "  'and',\n",
       "  'humans',\n",
       "  '.',\n",
       "  'Example',\n",
       "  'tasks',\n",
       "  'in',\n",
       "  'which',\n",
       "  'this',\n",
       "  'is',\n",
       "  'done',\n",
       "  'include',\n",
       "  'speech',\n",
       "  'recognition',\n",
       "  ',',\n",
       "  'computer',\n",
       "  'vision',\n",
       "  ',',\n",
       "  'translation',\n",
       "  'between',\n",
       "  'natural',\n",
       "  'languages',\n",
       "  ',',\n",
       "  'as',\n",
       "  'well',\n",
       "  'as',\n",
       "  'other',\n",
       "  'mappings',\n",
       "  'of',\n",
       "  'inputs',\n",
       "  '.',\n",
       "  'AI',\n",
       "  'applications',\n",
       "  'include',\n",
       "  'advanced',\n",
       "  'web',\n",
       "  'search',\n",
       "  'engines',\n",
       "  'e.g.',\n",
       "  ',',\n",
       "  'Google',\n",
       "  'Search',\n",
       "  ',',\n",
       "  'recommendation',\n",
       "  'systems',\n",
       "  'used',\n",
       "  'by',\n",
       "  'YouTube',\n",
       "  ',',\n",
       "  'Amazon',\n",
       "  'and',\n",
       "  'Netflix',\n",
       "  ',',\n",
       "  'understanding',\n",
       "  'human',\n",
       "  'speech',\n",
       "  'such',\n",
       "  'as',\n",
       "  'Siri',\n",
       "  'and',\n",
       "  'Alexa',\n",
       "  ',',\n",
       "  'self-driving',\n",
       "  'cars',\n",
       "  'e.g.',\n",
       "  ',',\n",
       "  'Waymo',\n",
       "  ',',\n",
       "  'automated',\n",
       "  'decision-making',\n",
       "  'and',\n",
       "  'competing',\n",
       "  'at',\n",
       "  'the',\n",
       "  'highest',\n",
       "  'level',\n",
       "  'in',\n",
       "  'strategic',\n",
       "  'game',\n",
       "  'systems',\n",
       "  'such',\n",
       "  'as',\n",
       "  'chess',\n",
       "  'and',\n",
       "  'Go',\n",
       "  '.',\n",
       "  'As',\n",
       "  'machines',\n",
       "  'become',\n",
       "  'increasingly',\n",
       "  'capable',\n",
       "  ',',\n",
       "  'tasks',\n",
       "  'considered',\n",
       "  'to',\n",
       "  'require',\n",
       "  'intelligence',\n",
       "  'are',\n",
       "  'often',\n",
       "  'removed',\n",
       "  'from',\n",
       "  'the',\n",
       "  'definition',\n",
       "  'of',\n",
       "  'AI',\n",
       "  ',',\n",
       "  'a',\n",
       "  'phenomenon',\n",
       "  'known',\n",
       "  'as',\n",
       "  'the',\n",
       "  'AI',\n",
       "  'effect',\n",
       "  '.',\n",
       "  'For',\n",
       "  'instance',\n",
       "  ',',\n",
       "  'optical',\n",
       "  'character',\n",
       "  'recognition',\n",
       "  'is',\n",
       "  'frequently',\n",
       "  'excluded',\n",
       "  'from',\n",
       "  'things',\n",
       "  'considered',\n",
       "  'to',\n",
       "  'be',\n",
       "  'AI',\n",
       "  ',',\n",
       "  'having',\n",
       "  'become',\n",
       "  'a',\n",
       "  'routine',\n",
       "  'technology',\n",
       "  '.',\n",
       "  'Artificial',\n",
       "  'intelligence',\n",
       "  'was',\n",
       "  'founded',\n",
       "  'as',\n",
       "  'an',\n",
       "  'academic',\n",
       "  'discipline',\n",
       "  'in',\n",
       "  '1956',\n",
       "  ',',\n",
       "  'and',\n",
       "  'in',\n",
       "  'the',\n",
       "  'years',\n",
       "  'since',\n",
       "  'has',\n",
       "  'experienced',\n",
       "  'several',\n",
       "  'waves',\n",
       "  'of',\n",
       "  'optimism',\n",
       "  ',',\n",
       "  'followed',\n",
       "  'by',\n",
       "  'disappointment',\n",
       "  'and',\n",
       "  'the',\n",
       "  'loss',\n",
       "  'of',\n",
       "  'funding',\n",
       "  'known',\n",
       "  'as',\n",
       "  'an',\n",
       "  'AI',\n",
       "  'winter',\n",
       "  'followed',\n",
       "  'by',\n",
       "  'new',\n",
       "  'approaches',\n",
       "  ',',\n",
       "  'success',\n",
       "  'and',\n",
       "  'renewed',\n",
       "  'funding',\n",
       "  '.',\n",
       "  'AI',\n",
       "  'research',\n",
       "  'has',\n",
       "  'tried',\n",
       "  'and',\n",
       "  'discarded',\n",
       "  'many',\n",
       "  'different',\n",
       "  'approaches',\n",
       "  'since',\n",
       "  'its',\n",
       "  'founding',\n",
       "  ',',\n",
       "  'including',\n",
       "  'simulating',\n",
       "  'the',\n",
       "  'brain',\n",
       "  ',',\n",
       "  'modeling',\n",
       "  'human',\n",
       "  'problem',\n",
       "  'solving',\n",
       "  ',',\n",
       "  'formal',\n",
       "  'logic',\n",
       "  ',',\n",
       "  'large',\n",
       "  'databases',\n",
       "  'of',\n",
       "  'knowledge',\n",
       "  'and',\n",
       "  'imitating',\n",
       "  'animal',\n",
       "  'behavior',\n",
       "  '.',\n",
       "  'In',\n",
       "  'the',\n",
       "  'first',\n",
       "  'decades',\n",
       "  'of',\n",
       "  'the',\n",
       "  '21st',\n",
       "  'century',\n",
       "  ',',\n",
       "  'highly',\n",
       "  'mathematical-statistical',\n",
       "  'machine',\n",
       "  'learning',\n",
       "  'has',\n",
       "  'dominated',\n",
       "  'the',\n",
       "  'field',\n",
       "  ',',\n",
       "  'and',\n",
       "  'this',\n",
       "  'technique',\n",
       "  'has',\n",
       "  'proved',\n",
       "  'highly',\n",
       "  'successful',\n",
       "  ',',\n",
       "  'helping',\n",
       "  'to',\n",
       "  'solve',\n",
       "  'many',\n",
       "  'challenging',\n",
       "  'problems',\n",
       "  'throughout',\n",
       "  'industry',\n",
       "  'and',\n",
       "  'academia',\n",
       "  '.'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ca0136",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c756d4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d55d00f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemTokens(tokens):\n",
    "    lst = []\n",
    "    for i in tokens: #Every individual token has to be lemmatized\n",
    "        lst.append(lemmatizer.lemmatize(i))\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ab25cb",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "###### Noise removal of special characters - punctuation marks..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce9295b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbd5955f",
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = dict((ord(i), None) for i in string.punctuation)\n",
    "# ord -> the inbuilt function in python for returning Unicode value of a corresponding character.\n",
    "# Each value is replaced with \"None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d3b3fe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{33: None,\n",
       " 34: None,\n",
       " 35: None,\n",
       " 36: None,\n",
       " 37: None,\n",
       " 38: None,\n",
       " 39: None,\n",
       " 40: None,\n",
       " 41: None,\n",
       " 42: None,\n",
       " 43: None,\n",
       " 44: None,\n",
       " 45: None,\n",
       " 46: None,\n",
       " 47: None,\n",
       " 58: None,\n",
       " 59: None,\n",
       " 60: None,\n",
       " 61: None,\n",
       " 62: None,\n",
       " 63: None,\n",
       " 64: None,\n",
       " 91: None,\n",
       " 92: None,\n",
       " 93: None,\n",
       " 94: None,\n",
       " 95: None,\n",
       " 96: None,\n",
       " 123: None,\n",
       " 124: None,\n",
       " 125: None,\n",
       " 126: None}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "280f1fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'~'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(126)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff433e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization and lemmatization\n",
    "def lemmer(text):\n",
    "    tokenized_text = nltk.word_tokenize(text.lower().translate(punct)) #Tokenization of text and Removal of punctuation\n",
    "    lemmatized_values = lemTokens(tokenized_text) # Calling the lemTokens function\n",
    "    return lemmatized_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e6912b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'have', 'a', 'headache']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test of lemmer function\n",
    "lemmer(\"I hav$$$$e a hea**dache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f68c45c",
   "metadata": {},
   "source": [
    "# Greeting - Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cdfa06bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "greeting_inputs = ['hi','hello','hey', 'greetings']\n",
    "greeting_responses = ['Hi, My name is ChatBot. How may I help you?', 'Hey, My name is ChatBot. How may I help you?','Hello, My name is ChatBot. How may I help you?','Greetings, My name is ChatBot. How may I help you?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8ba8f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greeting(text):\n",
    "    for token in text.split():\n",
    "        if token.lower() in greeting_inputs:\n",
    "            return random.choice(greeting_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "018ba4cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi, My name is ChatBot. How may I help you?'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing the greeting function\n",
    "greeting(\"Hello I need some help in JavaScript\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44cd870a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limitation - If we type an input \"Hello,\" like a greeting then followed comma, the chatbot won't recognize it as a greeting\n",
    "greeting(\"Hello, I need some help in Javacript\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a00650a",
   "metadata": {},
   "source": [
    "# Responses - Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "640803bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond(user_query):\n",
    "    bot_response = ''\n",
    "\n",
    "    # Tokenize the user query\n",
    "    sent_tokens, word_tokens = tokenize()\n",
    "    sent_tokens.append(user_query)\n",
    "\n",
    "    # Data encoding -> Converting sent_tokens to vectors\n",
    "    tfidf_obj = TfidfVectorizer(tokenizer = lemmer, stop_words = \"english\" )\n",
    "    tfidf = tfidf_obj.fit_transform(sent_tokens)\n",
    "\n",
    "\n",
    "    # Cosine similarity\n",
    "    # In this case tfidf = [t1, 12, t3, t4, user_query]\n",
    "    # Get the last element (user_query) which is in index [-1] and compare it with the entire list (t1, t2, t3, t4....)\n",
    "\n",
    "    sim_values = cosine_similarity(tfidf[-1],tfidf) # Cosine similarity of the last element with the entire list\n",
    "\n",
    "    # Selecting the token with masimum similarity value\n",
    "    # -2 means the second last response -> because the last response ([-1]) is the user query\n",
    "    index = sim_values.argsort()[0][-2] # Sorting values to give the index\n",
    "\n",
    "\n",
    "    flattened_sim = sim_values.flatten() #Flattening the sim_values to make them one dimensional\n",
    "    flattened_sim.sort() # Sorting flattened sim_values\n",
    "\n",
    "    required_tfidf = flattened_sim[-2]\n",
    "\n",
    "    if(required_tfidf ==0):\n",
    "        bot_response += \"Sorry, I cannot understand your question. I can only respond to questions in my corpus\"\n",
    "        return bot_response\n",
    "    else:\n",
    "        bot_response += bot_response + sent_tokens[index]\n",
    "        return bot_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a546c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('CHATBOT')\n",
    "flag = 1\n",
    "\n",
    "while (flag == 1):\n",
    "    user_query = input()\n",
    "    user_query = user_query.lower()\n",
    "\n",
    "    # If user wants to exit\n",
    "    if(user_query == 'exit'):\n",
    "        flag = 0\n",
    "        print('ChatBot : Bye! I\\'m glad to be of assistance to you:)')\n",
    "\n",
    "    else:\n",
    "        # If user enters a greeting\n",
    "        if(greeting(user_query)!= None):\n",
    "            print(\"Bot: \" +greeting(user_query))\n",
    "        else:\n",
    "            res = respond(user_query)\n",
    "            print(\"Chatbot: \", res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d30e012",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
